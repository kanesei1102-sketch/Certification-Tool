import streamlit as st
import pandas as pd
import numpy as np
from scipy import stats
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import scikit_posthocs as sp # 3ç¾¤ä»¥ä¸Šã®ãƒãƒ³ãƒ‘ãƒ©æ¯”è¼ƒç”¨

# --- æœ‰æ„å·®ãƒ©ãƒ™ãƒ«åˆ¤å®šç”¨é–¢æ•° ---
def get_sig_label(p):
    if p < 0.001: return "***"
    if p < 0.01: return "**"
    if p < 0.05: return "*"
    return "ns"

# 1. ãƒšãƒ¼ã‚¸æ§‹æˆ
with st.sidebar:
        # æ—¢å­˜ã®å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ãªã©ã®ã‚³ãƒ¼ãƒ‰...
        st.write("---") # åŒºåˆ‡ã‚Šç·š
        st.markdown("""
        ### ã€Notice / ã”æ¡ˆå†…ã€‘
        This tool is a beta version. If you plan to use results from this tool in your publications or conference presentations, **please contact the developer (Seiji Kaneko) in advance.**

        æœ¬ãƒ„ãƒ¼ãƒ«ã¯ç¾åœ¨é–‹ç™ºä¸­ã®ãƒ™ãƒ¼ã‚¿ç‰ˆã§ã™ã€‚è«–æ–‡æ²è¼‰ã‚„å­¦ä¼šç™ºè¡¨ç­‰ã«ä½¿ç”¨ã•ã‚Œã‚‹éš›ã¯ã€**äº‹å‰ã«é–‹ç™ºè€…ï¼ˆé‡‘å­ï¼‰ã¾ã§å¿…ãšä¸€å ±ãã ã•ã„ã€‚**

        ğŸ‘‰ **[Contact & Feedback Form / é€£çµ¡çª“å£](https://forms.gle/xgNscMi3KFfWcuZ1A)**

        We will provide guidance on validation support and proper acknowledgments/co-authorship.
        ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚µãƒãƒ¼ãƒˆã‚„ã€è¬è¾ãƒ»å…±è‘—ã®è¨˜è¼‰ã«ã¤ã„ã¦ã”æ¡ˆå†…ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚
        """)
st.set_page_config(page_title="Ultimate Stat Engine", layout="wide")
st.title("ğŸ”¬ Ultimate Scientific Stat Engine")
st.markdown("ãƒ‡ãƒ¼ã‚¿ã®æ€§è³ªã‚’è‡ªå‹•è¨ºæ–­ã—ã€æœ€é©ãªæ¤œå®šã¨å ±å‘Šç”¨ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚")

# 2. ã‚°ãƒ«ãƒ¼ãƒ—ç®¡ç†
if 'g_count' not in st.session_state: st.session_state.g_count = 3
c1, _ = st.columns([1, 4])
with c1:
    if st.button("ï¼‹ ã‚°ãƒ«ãƒ¼ãƒ—è¿½åŠ "): st.session_state.g_count += 1
    if st.session_state.g_count > 2 and st.button("ï¼ ã‚°ãƒ«ãƒ¼ãƒ—å‰Šé™¤"): st.session_state.g_count -= 1

st.divider()

# 3. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›
data_dict = {}
cols = st.columns(3)
for i in range(st.session_state.g_count):
    with cols[i % 3]:
        name = st.text_input(f"Group {i+1} Name", value=f"Group {i+1}", key=f"n{i}")
        raw = st.text_area(f"{name} ã®æ•°å€¤ (æ”¹è¡ŒåŒºåˆ‡ã‚Š)", key=f"d{i}", height=120)
        vals = [float(x.strip()) for x in raw.replace(',', '\n').split('\n') if x.strip()]
        if len(vals) >= 3: data_dict[name] = vals

# 4. è§£æã‚¨ãƒ³ã‚¸ãƒ³
if len(data_dict) >= 2:
    st.header("ğŸ“Š è§£æçµæœã¨æ¡ç”¨ç†ç”±")
    
    # --- è¨ºæ–­: æ­£è¦æ€§ã¨ç­‰åˆ†æ•£æ€§ ---
    all_normal = True
    shapiro_log = []
    for n, v in data_dict.items():
        _, p_s = stats.shapiro(v)
        all_normal &= (p_s > 0.05)
        shapiro_log.append(f"{n}(p={p_s:.4f})")
    
    _, p_lev = stats.levene(*data_dict.values())
    is_equal_var = (p_lev > 0.05)

    # åˆæœŸåŒ–
    method = ""
    p_final = 0.0
    p_disp = ""

    # A. 2ç¾¤æ¯”è¼ƒ
    if len(data_dict) == 2:
        gn = list(data_dict.keys())
        v1, v2 = data_dict[gn[0]], data_dict[gn[1]]
        if all_normal:
            method = "Student's t-test" if is_equal_var else "Welch's t-test"
            _, p_final = stats.ttest_ind(v1, v2, equal_var=is_equal_var)
        else:
            method = "Mann-Whitney U-test"
            _, p_final = stats.mannwhitneyu(v1, v2, alternative='two-sided')
        
        st.success(f"**æ¡ç”¨æ‰‹æ³•: {method}**")
        p_disp = f"{p_final:.2e}" if p_final < 0.001 else f"{p_final:.4f}"
        st.metric("P-value", p_disp)

    # B. 3ç¾¤ä»¥ä¸Š
    else:
        if all_normal and is_equal_var:
            method = "One-way ANOVA + Tukey's HSD"
            _, p_anova = stats.f_oneway(*data_dict.values())
            p_final = p_anova
            st.success(f"**æ¡ç”¨æ‰‹æ³•: {method}**")
            p_disp = f"{p_anova:.2e}" if p_anova < 0.001 else f"{p_anova:.4f}"
            st.write(f"å…¨ä½“På€¤ (ANOVA): **{p_disp}**")
            
            if p_anova < 0.05:
                flat_data = [v for sub in data_dict.values() for v in sub]
                labels = [n for n, sub in data_dict.items() for _ in sub]
                res = pairwise_tukeyhsd(flat_data, labels)
                df_res = pd.DataFrame(data=res._results_table.data[1:], columns=res._results_table.data[0])
                st.table(df_res)
        else:
            method = "Kruskal-Wallis test (ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯)"
            _, p_kw = stats.kruskal(*data_dict.values())
            p_final = p_kw
            st.warning(f"**æ¡ç”¨æ‰‹æ³•: {method}**")
            p_disp = f"{p_kw:.4f}"
            st.write(f"å…¨ä½“På€¤ (Kruskal-Wallis): **{p_disp}**")
            
            if p_kw < 0.05:
                st.write("å„ãƒšã‚¢ã®æ¯”è¼ƒ (Dunn's test):")
                df_dunn = sp.posthoc_dunn(list(data_dict.values()), p_adjust='bonferroni')
                df_dunn.columns = df_dunn.index = data_dict.keys()
                st.table(df_dunn)

    # --- è¨ºæ–­ãƒ­ã‚°ã®è¡¨ç¤º ---
    with st.expander("è©³ç´°ãªè¨ºæ–­ãƒ­ã‚° (å…ˆç”Ÿã¸ã®èª¬æ˜ç”¨)"):
        st.write(f"ãƒ»æ­£è¦æ€§åˆ¤å®š: {', '.join(shapiro_log)}")
        st.write(f"ãƒ»ç­‰åˆ†æ•£æ€§åˆ¤å®š: p = {p_lev:.4f}")

    # --- 5. åˆå¿ƒè€…ã§ã‚‚ã‚ã‹ã‚‹å ±å‘Šç”¨ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ ---
    st.divider()
    st.header("ğŸ“ ãã®ã¾ã¾ä½¿ãˆã‚‹å ±å‘Šç”¨ãƒ¬ãƒãƒ¼ãƒˆ")
    
    if all_normal and is_equal_var:
        easy_reason = "ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒãŒåã£ã¦ãŠã‚‰ãšã€ãƒãƒ©ãƒ„ã‚­ã‚‚å‡ä¸€ã ã£ãŸãŸã‚ã€æœ€ã‚‚æ¨™æº–çš„ã§ç²¾åº¦ã®é«˜ã„ã€tæ¤œå®š/ANOVAã€ã‚’é¸æŠã—ã¾ã—ãŸã€‚"
    elif not all_normal:
        easy_reason = "ãƒ‡ãƒ¼ã‚¿ã«æ¥µç«¯ãªåã‚Šã‚„å¤–ã‚Œå€¤ãŒè¦‹ã‚‰ã‚ŒãŸãŸã‚ã€æ•°å€¤ã®å¤§å°é–¢ä¿‚ï¼ˆé †ä½ï¼‰ã‚’é‡è¦–ã™ã‚‹ã€å¤–ã‚Œå€¤ã«å¼·ã„ã€ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šã€ã‚’é¸æŠã—ã¾ã—ãŸã€‚"
    else:
        easy_reason = "ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ©ãƒ„ã‚­ãŒç¾¤ã®é–“ã§ç•°ãªã£ã¦ã„ãŸãŸã‚ã€ãã®å·®ã‚’è£œæ­£ã—ã¦è¨ˆç®—ã™ã‚‹ã€Welchã®æ–¹æ³•ã€ã‚’é¸æŠã—ã¾ã—ãŸã€‚"

    is_significant = (p_final < 0.05)
    result_summary = "ã€æœ‰æ„å·®ã‚ã‚Šã€‘ã‚°ãƒ«ãƒ¼ãƒ—é–“ã«ã€å¶ç„¶ã¨ã¯è¨€ã„åˆ‡ã‚Œãªã„æ˜ã‚‰ã‹ãªå·®ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚" if is_significant else "ã€æœ‰æ„å·®ãªã—ã€‘ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®å·®ã¯ã€èª¤å·®ã®ç¯„å›²å†…ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚"

    report_text = f"""
ã€è§£æå ±å‘Šæ›¸ï¼š{", ".join(data_dict.keys())} ã®æ¯”è¼ƒã€‘

1. ã“ã®è§£æã§ä½•ã‚’ç¢ºèªã—ãŸã‹ï¼š
   å„ã‚°ãƒ«ãƒ¼ãƒ—ã®æ•°å€¤ã®å¹³å‡ã«ã€æ„å‘³ã®ã‚ã‚‹ã€Œé•ã„ã€ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’èª¿ã¹ã¾ã—ãŸã€‚

2. ã©ã®æ–¹æ³•ã§èª¿ã¹ãŸã‹ï¼ˆãã®ç†ç”±ï¼‰ï¼š
   æ¡ç”¨ã—ãŸæ‰‹æ³•ï¼š{method}
   ç†ç”±ï¼š{easy_reason}
   â€» é—‡é›²ã«è¨ˆç®—ã™ã‚‹ã®ã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿ã®å½¢ï¼ˆæ­£è¦æ€§ï¼‰ã‚„ãƒãƒ©ãƒ„ã‚­ï¼ˆç­‰åˆ†æ•£æ€§ï¼‰ã‚’äº‹å‰ã«ãƒã‚§ãƒƒã‚¯ã—ãŸä¸Šã§ã€æœ€ã‚‚ç§‘å­¦çš„ã«å¦¥å½“ãªæ‰‹é †ã‚’é¸ã‚“ã§ã„ã¾ã™ã€‚

3. è§£æã®çµæœï¼š
   åˆ¤å®šï¼š{result_summary}
   å…¨ä½“ã®På€¤ï¼š{p_disp}
   ï¼ˆâ€»På€¤ãŒ0.05ã‚ˆã‚Šå°ã•ã‘ã‚Œã°ã€çµ±è¨ˆå­¦çš„ã«ã€Œå·®ãŒã‚ã‚‹ã€ã¨åˆ¤æ–­ã—ã¾ã™ï¼‰

4. å€‹åˆ¥ã®é•ã„ï¼ˆå¤šé‡æ¯”è¼ƒï¼‰ï¼š
   {"3ç¾¤ä»¥ä¸Šã®æ¯”è¼ƒã®ãŸã‚ã€å„ãƒšã‚¢ã‚’ç·å½“ãŸã‚Šã§èª¿ã¹ã€å³ã—ã„åŸºæº–ã§æœ‰æ„å·®ã‚’åˆ¤å®šã—ã¾ã—ãŸã€‚è©³ç´°ã¯çµæœè¡¨ã®ãƒ©ãƒ™ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚" if len(data_dict) > 2 else "2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç›´æ¥æ¯”è¼ƒã—ã¾ã—ãŸã€‚"}

5. çµè«–ï¼š
   è§£æã®çµæœã€ä»Šå›ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã¯çµ±è¨ˆå­¦çš„ãªè£ä»˜ã‘ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚ã“ã®å†…å®¹ã«åŸºã¥ãã€æœ‰æ„å·®ãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ã—ãŸã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    """
    
    st.text_area("ä¸»æŸ»ã¸ã®èª¬æ˜ã‚„ã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒ¡ãƒ¢ã«ã‚³ãƒ”ãƒšã—ã¦ãã ã•ã„", value=report_text, height=400)

    # --- 6. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ ---
    st.download_button(
        label="ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜",
        data=report_text,
        file_name="statistical_report.txt",
        mime="text/plain"
    )

else:
    st.info("è§£æã‚’å§‹ã‚ã‚‹ã«ã¯ã€å„ã‚°ãƒ«ãƒ¼ãƒ—ã«3ã¤ä»¥ä¸Šã®æ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    # --- ç”»é¢ã®æœ€ä¸‹éƒ¨ã«å…è²¬äº‹é …ã‚’è¡¨ç¤º ---
    st.divider() # åŒºåˆ‡ã‚Šç·š
    st.caption("ã€å…è²¬äº‹é … / Disclaimerã€‘")
    st.caption("""
    æœ¬ãƒ„ãƒ¼ãƒ«ã¯çµ±è¨ˆå­¦çš„åˆ¤æ–­ãŠã‚ˆã³è§£æã®è£œåŠ©ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚
    è¨ˆç®—ã«ã¯ä¿¡é ¼æ€§ã®é«˜ã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ãŒã€æœ€çµ‚çš„ãªè§£é‡ˆãŠã‚ˆã³çµè«–ã«ã¤ã„ã¦ã¯ã€
    åˆ©ç”¨è€…ãŒå°‚é–€çš„çŸ¥è¦‹ã«åŸºã¥ã„ã¦åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚

    This tool is for assistive purposes. Final interpretations and conclusions 
    should be made by the user based on professional expertise.
    """)
